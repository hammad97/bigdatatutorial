{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d85ca91",
   "metadata": {},
   "source": [
    "## Exercise 1_a\n",
    "* 312441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc40bc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\spark-3.3.0-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is required to run everytime because sometimes, \n",
    "# without running it it fails to establish connection back to python.\n",
    "import findspark\n",
    "import matplotlib.pyplot as plt\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e4e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType, DecimalType\n",
    "from decimal import Decimal\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import mean , stddev, col, to_date, date_format\n",
    "import pyspark.sql.functions as F\n",
    "from time import strptime\n",
    "import datetime\n",
    "from ast import literal_eval\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0079d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apache spark basic configuration and declaring required variables\n",
    "conf = pyspark.SparkConf().setAppName('ex8_1a').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf = conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "a = ['spark', 'rdd', 'python', 'context', 'create', 'class']\n",
    "b = ['operation', 'apache', 'scala', 'lambda', 'parallel', 'partition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76ef022f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', 1),\n",
       " ('rdd', 1),\n",
       " ('python', 1),\n",
       " ('context', 1),\n",
       " ('create', 1),\n",
       " ('class', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating two RDD objects to later perform join operations.\n",
    "rdd1 = sc.parallelize(a)\n",
    "rdd2 = sc.parallelize(b)\n",
    "rdd1=rdd1.map(lambda x: (x,1))\n",
    "rdd2=rdd2.map(lambda x: (x,1))\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b44f608e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('operation', 1),\n",
       " ('apache', 1),\n",
       " ('scala', 1),\n",
       " ('lambda', 1),\n",
       " ('parallel', 1),\n",
       " ('partition', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d6aa639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fullOuterJoin: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('python', (1, None)),\n",
       " ('class', (1, None)),\n",
       " ('scala', (None, 1)),\n",
       " ('parallel', (None, 1)),\n",
       " ('partition', (None, 1)),\n",
       " ('spark', (1, None)),\n",
       " ('rdd', (1, None)),\n",
       " ('context', (1, None)),\n",
       " ('create', (1, None)),\n",
       " ('operation', (None, 1)),\n",
       " ('apache', (None, 1)),\n",
       " ('lambda', (None, 1))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. \n",
    "# Performing fullOuterJoin and rightOuterJoin function using code below\n",
    "rdd3 = rdd1.fullOuterJoin(rdd2)\n",
    "print('fullOuterJoin: ')\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96058299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rightOuterJoin: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('scala', (None, 1)),\n",
       " ('parallel', (None, 1)),\n",
       " ('partition', (None, 1)),\n",
       " ('operation', (None, 1)),\n",
       " ('apache', (None, 1)),\n",
       " ('lambda', (None, 1))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('rightOuterJoin: ')\n",
    "rdd4 = rdd1.rightOuterJoin(rdd2)\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc511f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', 1),\n",
       " ('rdd', 1),\n",
       " ('python', 1),\n",
       " ('context', 1),\n",
       " ('create', 1),\n",
       " ('class', 1),\n",
       " ('operation', 1),\n",
       " ('apache', 1),\n",
       " ('scala', 1),\n",
       " ('lambda', 1),\n",
       " ('parallel', 1),\n",
       " ('partition', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.\n",
    "# Combining both rdd to count number of s \n",
    "rdd6=sc.union([rdd1,rdd2])\n",
    "rdd6.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f674920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(map and reduce) Occurences of s :   [('s', 4)]\n"
     ]
    }
   ],
   "source": [
    "# using map reduce() we count 's' occurences\n",
    "mappper = rdd6.map(lambda s: (\"s\", s[0].count('s')))\n",
    "reducer=mappper.reduceByKey(lambda x,y: x+y)\n",
    "print('(map and reduce) Occurences of s :  ', reducer.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d016532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(aggregate) Occurences of s :   4\n"
     ]
    }
   ],
   "source": [
    "# using aggregrate() we count s occurences\n",
    "number_of_s=rdd6.aggregate(0, lambda i, x: i + x[0].count('s'), lambda i, j: i+j)\n",
    "print('(aggregate) Occurences of s :  ', number_of_s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
